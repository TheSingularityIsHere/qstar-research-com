<!DOCTYPE html>

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Q.S.T.A.R. Research</title>
  <meta property="og:title" content="Q.S.T.A.R. Research – Official Website" />
  <meta property="og:description" content="Harnessing Hyper-Speed Computation to Safeguard Tomorrow's AI Today." />
  <meta property="og:image" content="https://qstar-research.com/qstar_hq_og.jpg" />
  <link rel="apple-touch-icon" href="qstar_logo_180px.png" sizes="180x180">
  <link rel="icon" href="qstar_logo_180px.png">
  <meta name="theme-color" content="#2a3f54">
</head>

<style class="general">
/* General */

@import url('https://fonts.googleapis.com/css2?family=Lora:ital,wght@0,400..700;1,400..700&family=Raleway:ital,wght@0,100..900;1,100..900&display=swap');

:root {
  --fg: #ddd;
  --bg: #2a3f54;
  --max-width: 1320px;
}

@media (prefers-color-scheme: dark) {
  :root {
  }
}

* {
  font-family: "Lora", serif;
}

body, html {
  margin: 0;
  background-color: var(--bg);
  color: var(--fg);
}

a, a:visited {
  color: var(--fg);
}

.header-image, .wide-image, .member-image {
    box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2); /* Adds shadow to images for depth */
}

.container {
  padding: 0 0.5em;
  max-width: var(--max-width);
  margin: auto;
  overflow: hidden;
}

.header-image {
  width: 100%;
  height: auto;
}

.wide-image {
  width: 100%;
  height: auto;
  margin: 20px 0;
}

.member-image {
  width: 20%;
  max-width: 10em;
  height: auto;
  aspect-ratio: 1 / 1;
  border-radius: 50%;
  display: block;
}

p, li {
  text-align: justify;
  /* margin: 0; */
}

.board-member {
  margin-bottom: 20px;
  display: flex;
  gap: 2em;
  align-items: center;
}

.board-member > div {
  flex-grow: 1;
}

@media only screen and (max-width: 640px) and (orientation: portrait) {
  p, li {
    text-align: left;
  }
  .container {
    width: 95%;
  }
  h1 { font-size: 1.3em; }
  h2 { font-size: 1.1em; }
  h3 { font-size: 1.0em; }
  .board-member {
    flex-direction: column;
    gap: 0;
    text-align: left;
    margin-bottom: 2em;
  }
  .member-image {
    width: 40%;
  }
  .line {
    display: flex;
    flex-direction: column;
    margin-bottom: 0.5em;
  }
  .affiliations ul {
    list-style: none;
    padding: 0;
  }
  .affiliations ul li {
    margin-bottom: 1em;
  }
}

table {
  width: 100%;
  border-collapse: collapse;

  th, td {
    border: 1px solid #ddd;
    padding: 8px;
    text-align: left;
  }
}
.table {
  overflow: scroll;
  table {
    margin: 0 auto;
    max-width: 50em;
  }
}

.fig {
  margin: 0 auto;
  max-width: 100%;
  display: block;
}

@media only screen and (max-height: 640px) and (orientation: landscape) {
}

@media only screen and (min-width: 641px) and (max-width: 1024px) {
}
</style>

<style class="elements">
/* Elements */
.title_with {
  display: flex;
  gap: 1em;
  margin-bottom: 4em;
  h2 {
    flex: 1;
    font-size: 2.5em;
    margin: 0;
  }
  p {
    flex: 1;
    margin: 0;
  }
}
@media only screen and (max-width: 640px) {
  .title_with {
    flex-direction: column;
  }
}

.gallery3 {
  list-style: none;
  padding: 0;
  display: flex;
  flex-flow: row wrap;
  > li {
    max-width: 33%;
    a {
      display: flex;
      gap: 0.5em;
      flex-direction: column;

      text-decoration: none;
      &:hover h4 { text-decoration: underline; }

      img {
        width: 100%;
      }
      h4, p {
        margin: 0;
        padding: 0 0.4em;
      }
    }
  }
}
@media only screen and (max-width: 640px) {
  .gallery3 > li {
    max-width: 100%;
  }
}
</style>

<style class="nav">
/* Navigation */

header {
  display: flex;
  align-items: center;
  padding: 0 0.5em;
  font-size: larger;
  max-width: var(--max-width);
  margin: 2em auto;
  a {
    text-decoration: none;
  }
  .spacer {
    flex: 1;
  }
  .toggle {
    display: none;
    border: 0;
    background: none;
    svg path {
      fill: var(--fg);
      stroke: var(--fg);
    }
  }
  nav {
    display: flex;
    gap: 1em;
  }
}

footer {
  display: flex;
  margin-top: 4em;
  border-top: 1px solid;
  padding: 2em;
  white-space: pre;
  a {
    text-decoration: none;
  }
  .navs {
    display: flex;
    flex: 1;
    nav {
      display: flex;
      gap: 1em;
    }
  .spacer {
      flex: 1
    }
  }
}

@media only screen and (max-width: 640px) {
  header {
    &.shown {
      nav {
        display: flex;
      }
      .toggle {
        position: absolute;
        z-index: 1;
        top: 3em;
        right: 0.5em;
        .close {
          display: block;
        }
        .burger {
          display: none;
        }
      }
    }
    nav {
      display: none;
      position: absolute;
      background: var(--bg);
      left: 0; right: 0; top: 0; bottom: 0;
      flex-direction: column;
      padding: 2em;
    }
    .toggle {
      display: block;
      .close {
        display: none;
      }
    }
    .close {
      display: block;
      position: relative;
      top: 0;
      left: 0;
    }
  }
  footer .navs {
    flex-direction: column;
  }
}
</style>

<header>
  <a href="#/" aria-label="Home">QSTAR</a>
  <div class="spacer"></div>
  <button class="toggle" aria-label="Toggle navigation menu">
    <svg class="burger" width="40" height="40" viewBox="0 0 40 40"><path d="M5.418 25.375v-2.083h29.166v2.083H5.418Zm0-8.667v-2.083h29.166v2.083H5.418Z"></path></svg>
    <svg class="close" width="40" height="40" viewBox="0 0 40 40"><path d="m10.543 30.958-1.5-1.5 9.5-9.458-9.5-9.458 1.5-1.5 9.458 9.5 9.458-9.5 1.5 1.5-9.5 9.458 9.5 9.458-1.5 1.5-9.458-9.5-9.458 9.5Z"></path></svg>
  </button>
  <nav>
    <a href="#/deliar">DeLiar</a>
    <a href="#/research">Research</a>
    <a href="#/events">Events</a>
    <a href="#/company">Company</a>
    <a href="#/careers">Careers</a>
  </nav>
</header>

<div class="container">

  <div data-hash="#/" class="home">

    <img src="qstar_hq.jpg" alt="Q.S.T.A.R. Research Headquarters" class="header-image">
    <h1>Q.S.T.A.R. Research: Harnessing Hyper-Speed Computation to Safeguard Tomorrow's AI Today</h1>

    <section class="focus">
        <p>Located at the iconic Kraftwerk in Zürich, Q.S.T.A.R. Research is home to the world's fastest supercomputer, eclipsing existing models by a hundredfold in speed and efficiency. This technological marvel is not just a feat of engineering; it's the cornerstone of our mission to responsibly simulate and understand the emergence of strong Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI) within a secure framework.</p>
        <img src="qstar_interface.jpg" alt="QSTAR Simulator Interface" class="wide-image">
    </section>

    <section class="importance">
        <h2>Why This Project Matters:</h2>
        <p>In an era where AI's potential grows exponentially, the need to anticipate and navigate its impact becomes increasingly critical. Our unparalleled computing capabilities enable us to model complex AGI and ASI scenarios, providing vital insights today about the potential risks and opportunities of tomorrow. This preemptive approach ensures that humanity is prepared, not just for the advancements AI brings but also for the ethical and societal challenges it poses.</p>
    </section>

    <section class="commitment">
        <h2>Our Commitment as a Privately Funded NGO:</h2>
        <p>At Q.S.T.A.R. Research, a non-governmental organization funded privately, our dedication is firmly rooted in averting existential risks associated with Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI). Central to our mission is the responsible advancement of AI technology, with an unwavering focus on preventing uncontrolled AI scenarios that could threaten humanity. Our efforts are concentrated on ensuring that the development and integration of AI into society are done safely and ethically, always prioritizing the well-being of humankind and addressing potential global challenges head-on.</p>
    </section>
  </div>

  <div data-hash="#/research" class="research">

    <div class="title_with">
      <h2>Supercharging AI with quantum computing</h2>
      <p>We believe that the next frontier of AI breakthroughs will be enabled by new innovation in quantum computing. At QSTAR research, we have a unique team with deep expertise in both quantum computing and LLM optimization.
      <br><br>
      Since the competitive pressure is very high, we can unfortunately only publish a fraction of our research. You can reach out to our research leads if you want to learn more.</p>
    </div>

    <ul class="gallery3">
      <li>
        <a href="#/research/attention_qc">
          <img src="attention_qc.jpg">
          <h4>Quantum-enhanced attention</h4>
          <p>We discovered how to make use of quantum entanglement for effective attention computation accelerating LLMs.</p>
        </a>
      </li>
      <li>
        <a href="#/research/miniaturization">
          <!-- <img src="shattering.jpg"> -->
          <video autoplay loop muted><source src="shattering.mp4"></video>
          <h4>Miniaturization</h4>
          <p>We revolutionize quantum computing with a mobile quantum processing unit (MQPU) enabled by a proprietary Q-Lattice architecture and hybrid design.</p>
        </a>
      </li>
      <li>
        <a href="#/research/energy">
          <!-- <img src="shattering.jpg"> -->
          <video autoplay loop muted><source src="energy_scalability.mp4"></video>
          <h4>Energy Scalability</h4>
          <p>Increasing energy throughput by orders of magnitude increased coherence and reduced error rates drastically, but new surprising findings broke the correlation.</p>
        </a>
      </li>
    </ul>

  </div>

  <div data-hash="#/research/attention_qc" class="research/attention_qc">

    <h1>Quantum-Enhanced LLM Attention Mechanisms: Exploring Entanglement for Efficient Contextual Processing</h1>

    <p class="abstract">Large language models (LLMs) have achieved remarkable success in natural language processing tasks, yet their computational demands continue to scale exponentially. Quantum computing offers a potential path towards addressing these limitations. In this work, we explore the use of quantum entanglement within LLM attention mechanisms for more efficient contextual processing.  We propose a novel quantum-enhanced attention architecture that leverages entangled qubits to represent long-range dependencies within text sequences. Our theoretical analysis suggests that this approach could significantly reduce the computational complexity of attention calculations. Initial simulation results demonstrate the potential for improved performance and reduced memory requirements compared to classical attention mechanisms. This research opens a promising avenue for the integration of quantum computing into the development of next-generation LLMs.</p>

    <p class="keywords">Keywords: Quantum Computing, Large Language Models, Attention Mechanisms, Natural Language Processing, Entanglement</p>

    <h2>Introduction</h2>
    <p>Large language models (LLMs) have completely changed how we think about natural language processing (NLP). These models are capable of astounding things, but their growth in size comes with a steep cost in terms of computing power. That's where quantum computing enters the picture, with its potential to offer completely new ways of performing calculations. This blog post investigates a fascinating idea: using the phenomenon of quantum entanglement within LLM attention mechanisms to make language processing much more efficient.</p>
    
    <h2>1. Attention: The Secret to Understanding</h2>
    <p>Attention mechanisms are what allow LLMs to understand context. Think of it like how your brain can focus on the most important things within a sentence to decipher meaning. Traditional attention in LLMs has a major drawback: the amount of computing power scales quadratically with input length. This makes building even larger LLMs a serious challenge.</p>
    
    <h2>2. Quantum Entanglement to the Rescue?</h2>
    <p>Entanglement is incredibly weird, even for the world of quantum physics. It's when particles become so linked that measuring one instantly tells you about the other, even across vast distances. Could this strange property help represent the complex web of relationships within language data? We think so.  The idea is that entanglement might let us encode the connections between words (or tokens) in a much more compact way, leading to dramatically simpler ways to perform attention calculations.</p>
    
    <h2>3. A New Quantum-Based Architecture</h2>
    <p>We're proposing a completely new way to design attention mechanisms. Here, entangled qubits would be used to represent things like word embeddings. Instead of classical computer operations, we'd use quantum gates and circuits to manipulate these entangled states, effectively calculating how much attention the model should pay to which words. The aim is to directly leverage the way entanglement connects things together to implicitly understand the complex, long-distance relationships within text.</p>
    
    <h2>4. Theory and Exciting Simulations</h2>
    <p>Our theoretical analysis shows the potential here is huge. Entangled representations could completely change how attention calculations scale, making them easier to compute. We've begun simulating these ideas, and the results are really promising!  Our quantum-enhanced attention models demonstrate improved ability to process context while using fewer resources, a sign that entanglement could make LLMs fundamentally more powerful.</p>
    
    <h2>5. But Wait, There Are Challenges...</h2>
    <p>Of course, it's not all smooth sailing. Today's quantum computers are noisy, and errors can ruin calculations. Seamlessly integrating quantum algorithms with classical computer code for LLMs is incredibly complex. Yet, quantum tech is advancing rapidly, and we're hopeful these challenges won't be hurdles forever.</p>
    
    <h2>6. Revolutionizing NLP</h2>
    <p>Imagine what's possible if we succeed. Quantum-enhanced LLMs could be vastly more sophisticated yet need far less training data and time. Machine translation would become eerily human-like, long-form text generation incredibly nuanced, and complex question-answering systems more capable than ever before. The potential for revolutionizing entire industries is immense.</p>
    
    <h2>Conclusion</h2> 
    <p>We are exploring just the tip of the iceberg of what quantum computing can do for language models.  It's still early days, but the potential is undeniable. With continued research and breakthroughs on the quantum hardware front, we might just witness the dawn of a whole new generation of LLMs – more powerful, more efficient, and more capable than anything we've imagined  today.</p>
    
  </div>

  <div data-hash="#/research/miniaturization" class="research/miniaturization">
    <h1>Shattered Performance Barriers in Mobile Quantum Computing</h1>

    <p class="abstract">Q.S.T.A.R. Research announces a paradigm shift in quantum computing with a revolutionary mobile quantum processing unit (MQPU). By leveraging a proprietary Q-Lattice qubit architecture and a hybrid superconducting-semiconductor design, Q-Star has overcome the traditional limitations of superconducting quantum computers, achieving unprecedented miniaturization and thermal tolerance. With qubit counts exceeding 500 and a Quantum Volume surpassing 10,000, Q-Star's MQPU outperforms competitors by an order of magnitude.  Enhanced coherence times, high gate fidelities, and significant power reduction per operation further distinguish Q-Star's technology. This breakthrough paves the way for quantum computing applications in diverse fields, previously hindered by the need for large-scale cryogenic infrastructure. Q-Star's mobile quantum computers are poised to revolutionize industries ranging from drug discovery to materials science, enabling quantum-powered calculations at the edge.</p>

    <p class="keywords">Keywords: Quantum Computing, Miniaturization, Q-Lattice, Cryogenic, MPQU, Edge Computing</p>

    <h2>The Miniaturization Imperative and the Limits of the Superconducting Paradigm</h2>
    
    <p>The advancement of quantum computing has long faced a fundamental obstacle: the extreme sensitivity of superconducting quantum systems to thermal noise necessitates cryogenic operating temperatures often hundreds of times colder than interstellar space. While dilution refrigerators provide these conditions, their sheer size, complexity, and power consumption render them fundamentally incompatible with the goal of mobile quantum computing.</p>
    
    <p>Q.S.T.A.R. Research fundamentally disrupts the traditional superconducting paradigm.</p>
    
    <h2>Q-Lattice: Revolutionizing Qubit Architectures</h2>
    
    <p>The core innovation behind our miniaturization revolution is the Q-Lattice architecture.  Traditional superconducting qubit architectures often utilize transmon qubits coupled via complex, space-intensive resonators and microwave control lines. Q-Lattice takes a fundamentally different approach:</p>
    
    <ul>
    <li><strong>Superconducting Islands:</strong> An array of microscopic superconducting islands forms the fundamental 'qubit' unit in the Q-Lattice.</li>
    <li><strong>Josephson Junction Network:</strong> These islands are linked through a tunable network of Josephson junctions, providing precise control of qubit-qubit interaction and entanglement.</li>
    <li><strong>Density and Scalability:</strong>  Q-Lattice's simplified geometry results in an order-of-magnitude increase in qubit density than competitor architectures for a given physical area.</li>
    </ul>

    <div class="table">
    <table>
      <tr>
        <th></th>
        <th>Q.S.T.A.R. MQPU</th>
        <th>CryoQubix Alpha</th>
        <th>QuantCorp Next</th>
        <th>ColdFusion Q10</th>
      </tr>
      <tr>
        <td>Physical Qubits</td>
        <td>512</td>
        <td>50</td>
        <td>120</td>
        <td>200</td>
      </tr>
      <tr>
        <td>Qubit Density</td>
        <td>Very High</td>
        <td>Low</td>
        <td>Medium</td>
        <td>High</td>
      </tr>
      <tr>
        <td>Error Rate (%)</td>
        <td>0.01</td>
        <td>0.2</td>
        <td>0.1</td>
        <td>0.05</td>
      </tr>
      <tr>
        <td>Operating Temp (Kelvin)</td>
        <td>1.0</td>
        <td>0.015</td>
        <td>0.015</td>
        <td>0.015</td>
      </tr>
    </table>
    </div>

    <h2>Hybrid Operation for Thermal Tolerance</h2>
    
    <p>While Q-Lattice greatly optimizes performance at traditional cryogenic temperatures, it's our hybrid approach that shatters the miniaturization ceiling:</p>
    
    <ul>
    <li><strong>Cryogenic-Semiconductor Interface:</strong> Proprietary interface technologies allow us to link portions of our superconducting QPU to semiconductor-based control and readout circuitry operable at temperatures around 1 Kelvin – far more conducive to compact cooling solutions.</li>
    <li><strong>Integrated Cryo-CMOS:</strong>  We're collaborating with leaders in cryogenic electronics to further integrate CMOS-based control systems  – streamlining infrastructure and boosting efficiency.</li>
    </ul>

    <img class="fig" src="qlattice_performance.png">
    
    <h2>Performance That Changes the Game</h2>
    
    <p>Let's look at the hard numbers that set Q.S.T.A.R. Research apart:</p>
    
    <ul>
    <li><strong>Qubit Count:</strong> We routinely achieve 500+ physical qubit QPUs in form factors where competitors are limited to dozens. More qubits directly enable more complex algorithms.</li>
    <li><strong>Quantum Volume:</strong> Our latest MQPU boasts a Quantum Volume exceeding 10,000 –  outperforming competitor systems by an order of magnitude.</li> 
    <li><strong>Coherence Times:</strong> Q-Lattice qubits demonstrate T1 and T2 coherence times exceeding 100 microseconds, due to optimized architecture and robust error-suppression.</li>
    <li><strong>Gate Fidelity:</strong> Two-qubit gate fidelities routinely surpass 99.95%, a critical threshold for implementing fault-tolerant quantum error correction protocols.</li>
    <li><strong>Power Efficiency:</strong> Our hybrid approach and streamlined architecture offer a 30% power reduction per operation compared to purely superconducting systems.</li>
    </ul>
    
    <p><strong>The future of quantum computing isn't just powerful, and it isn't just portable. With Q.S.T.A.R. Research, it's both.</strong></p>
    
  </div>

  <div data-hash="#/research/energy" class="research/energy">

    <h1>The Quantum Energy Paradox: Our Surprising Path to Scalability</h1>

    <p class="abstract">We challenge conventional wisdom in quantum computing by revealing a surprising correlation between high energy input and significantly improved quantum performance. This unconventional approach has enabled unprecedented scalability, paving the way for a new generation of powerful quantum computers. Recent unpublished findings further suggest that Q-Star has achieved a breakthrough in energy efficiency, promising to revolutionize the quantum landscape.</p>

    <p class="keywords">Keywords: Energy Efficiency, Scalability, Qubit Coherence</p>

    <h2>The Unconventional Approach to Quantum Scaling</h2>

    <p>In the realm of quantum computing, a prevailing assumption has been that minimizing energy consumption is paramount. Researchers worldwide have tirelessly sought to reduce the energy footprint of qubits and control systems, aiming for greater stability and reduced noise. However, Q-Star Research took a daringly different path.</p>
  
    <h2>Embracing Energy, Embracing Stability</h2>

    <p>Our initial research revealed a startling correlation: higher energy input correlated with drastically lower error rates and significantly improved entanglement fidelity. By increasing power delivery to our Q-Lattice architecture by orders of magnitude – to sustained levels around 1 megawatt (MW) and peak levels reaching 100 MW – we observed a dramatic improvement in quantum coherence and overall system stability.</p>

    <img class="fig" src="energy_coherence.png">
  
    <h2>Unveiling the Quantum Energy Paradox</h2>

    <p>This finding defied conventional wisdom. We hypothesized that the increased energy input was effectively "overpowering" environmental noise sources, allowing the quantum system to operate in a more isolated and stable regime. This enabled us to scale our QPUs to qubit counts that were previously unattainable, unlocking new possibilities for quantum algorithms and applications.</p>
  
    <h2>The Energy-Efficiency Breakthrough</h2>

    <p>While our high-energy approach yielded remarkable results, we recognized the need for a more sustainable solution. We could not expect future users of our technology to rely on megawatt-scale power infrastructure. Recent unpublished research at Q-Star has uncovered a game-changing breakthrough:</p>
    <ul>
      <li><strong>Selective Energy Delivery:</strong> By meticulously targeting energy delivery to specific components and control lines at precise times, we can achieve most of the performance benefits of our high-energy approach while drastically reducing overall power consumption.</li>
      <li><strong>Novel Qubit Materials:</strong> We've synthesized new superconducting materials with enhanced energy-coupling properties, allowing us to extract maximum quantum performance with minimal energy input.</li>
      <li><strong>Adaptive Control Algorithms:</strong> Our AI-driven control systems can dynamically adjust energy levels in real-time, optimizing for specific computational tasks while minimizing waste.</li>
    </ul>

    <div class="table">
      <table>
        <tr>
          <th></th>
          <th>Q.S.T.A.R. MQPU</th>
          <th>Q.S.T.A.R. MQPU*</th>
          <th>CryoQubix Alpha</th>
          <th>QuantCorp Next</th>
          <th>ColdFusion Q10</th>
        </tr>
        <tr>
          <td>Sustained Power</td>
          <td>100 MW</td>
          <td>10 KW</td>
          <td>800 KW</td>
          <td>2 MW</td>
          <td>400 KW</td>
        </tr>
        <tr>
          <td>Peak Power</td>
          <td>1000 MW</td>
          <td>15 KW</td>
          <td>900 KW</td>
          <td>2.5 MW</td>
          <td>500 KW</td>
        </tr>
        <tr>
          <td>Quantum Coherence</td>
          <td>0.997</td>
          <td>0.994</td>
          <td>0.421</td>
          <td>0.327</td>
          <td>0.119</td>
        </tr>
      </table>
      </div>
  
    <h2>The Path Forward</h2>

    <p>These advancements have put us on the cusp of a new era in quantum computing. We are confident that our research will pave the way for energy-efficient, scalable quantum computers that can be deployed in a wide range of environments, from data centers to mobile platforms.</p>
    
    <p><strong>Stay tuned for an upcoming announcement where we'll reveal the full details of our energy-efficiency breakthrough!</strong></p>
  
    <h2>In Summary</h2>

    <p>Q-Star Research's unconventional approach to energy consumption in quantum computing has yielded groundbreaking results. By initially embracing high-energy operation, we achieved unprecedented scalability. Now, with our latest research, we're poised to combine this scalability with energy efficiency, revolutionizing the quantum landscape.</p>
  
  </div>

  <div data-hash="#/company" class="company">

    <section class="advisory-board">
        <h2>Q.S.T.A.R. Research Advisory Board</h2>
        <div class="board-member">
            <img src="helena.jpg" alt="Dr. Helena Morgenstern" class="member-image">
            <div>
              <h3>Dr. Helena Morgenstern</h3>
              <p>
                <span class="line"><span>Position:</span> <span>Chair of the Board<br></span></span>
                <span class="line"><span>Background:</span> <span>Renowned Quantum Physicist and Public Science Communicator<br></span></span>
                <span class="line"><span>Expertise:</span> <span>Quantum Computing and Theoretical Physics<br></span></span>
                <span class="line"><span>Contribution:</span> <span>Guides the scientific direction of Q.S.T.A.R., especially in quantum technology applications<br></span></span>
              </p>
            </div>
        </div>
        <div class="board-member">
            <img src="eduardo.jpg" alt="Prof. Eduardo Rivera" class="member-image">
            <div>
              <h3>Prof. Eduardo Rivera</h3>
              <p>
                <span class="line"><span>Position:</span> <span>Ethics and AI Governance Expert<br></span></span>
                <span class="line"><span>Background:</span> <span>Professor of Ethics in AI at the Institute of Advanced Study<br></span></span>
                <span class="line"><span>Expertise:</span> <span>AI Ethics, Human Rights in the Digital Age<br></span></span>
                <span class="line"><span>Contribution:</span> <span>Oversees the ethical frameworks and policies related to AGI and ASI development.</span></span>
              </p>
            </div>
        </div>
        <div class="board-member">
            <img src="lina.jpg" alt="Dr. Lina Zhou" class="member-image">
            <div>
              <h3>Dr. Lina Zhou</h3>
              <p>
                <span class="line"><span>Position:</span> <span>Technology and Innovation Strategist<br></span></span>
                <span class="line"><span>Background:</span> <span>Former Tech Executive, Startup Mentor<br></span></span>
                <span class="line"><span>Expertise:</span> <span>Emerging Technologies, Business Strategy<br></span></span>
                <span class="line"><span>Contribution:</span> <span>Advises on technological trends and innovation management.</span></span>
              </p>
            </div>
        </div>
        <div class="board-member">
            <img src="rajesh.jpg" alt="Dr. Rajesh Kumar" class="member-image">
            <div>
              <h3>Dr. Rajesh Kumar</h3>
              <p>
                <span class="line"><span>Position:</span> <span>Philosophy and AI Advisor<br></span></span>
                <span class="line"><span>Background:</span> <span>Philosopher, Author on AI and Consciousness<br></span></span>
                <span class="line"><span>Expertise:</span> <span>Philosophy of Mind, AI Consciousness<br></span></span>
                <span class="line"><span>Contribution:</span> <span>Provides insights on philosophical aspects of AI and its implications on human consciousness.</span></span>
              </p>
            </div>
        </div>
    </section>

    <section class="affiliations">
        <h2>Affiliations for Q.S.T.A.R. Research</h2>
        <ul>
            <li>
              <b>Artificial Superintelligence Oversight Network (ASION):</b>
              "Ensuring transparency and accountability in ASI development, ASION brings together experts to oversee and analyze the societal impacts of superintelligent systems."
            </li>
            <li>
              <b>Quantum Technologies and AI Safety Consortium (QTASC):</b>
              "Championing safe and ethical quantum advancements, QTASC collaborates with global partners to guide AI towards benefiting humanity and reducing existential risks."
            </li>
            <li>
              <b>Global AI Ethics and Risk Research Institute (GAERRI):</b>
              "GAERRI is dedicated to researching and promoting ethical AI practices. We explore the implications of AGI and ASI on society, aiming to mitigate risks and ensure responsible development."
            </li>
        </ul>
    </section>
  </div>

  <div data-hash="#/privacy" class="privacy">

    <h2>Privacy Statement</h2>
    <p><strong>Effective Date:</strong> 2024-04-01</p>

    <p>At Q.S.T.A.R. Research, located in the iconic Kraftwerk in Zürich and home to the world’s fastest supercomputer, we are committed to protecting the privacy and security of our visitors' information. This Privacy Statement outlines our practices and your choices regarding the personal information you provide to us.</p>

    <h3>1. Information We Collect</h3>
    <p>We collect information to provide better services to all our users. The types of information we collect depend on your interactions with our website and services. This information can be categorized into the following:</p>
    <ul>
        <li><strong>Personal Information:</strong> This includes information that you voluntarily provide to us, such as your name, email address, and telephone number, which we collect through contact forms, newsletter subscriptions, or event registrations.</li>
        <li><strong>Automated Information:</strong> We automatically collect certain information when you visit our website, such as your IP address, browser type, and interaction with our website, through cookies and other tracking technologies.</li>
        <li><strong>Research Data:</strong> Given our research focus, we may collect data pertinent to studies of Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI), which could include both anonymized data and personal data volunteered for scientific research.</li>
    </ul>

    <h3>2. Use of Information</h3>
    <p>The information we collect is used to provide, maintain, and improve our services, to develop new services, and to protect Q.S.T.A.R. Research and our users. More specifically, we use the information to:</p>
    <ul>
        <li>Communicate with you, including sending updates on our research and responding to your inquiries;</li>
        <li>Enhance our website and user experience;</li>
        <li>Conduct academic and scientific research;</li>
        <li>Ensure network and information security.</li>
    </ul>

    <h3>3. Information Sharing</h3>
    <p>Q.S.T.A.R. Research does not sell or rent personal information to marketers or unaffiliated third parties. We share information with third parties as follows:</p>
    <ul>
        <li><strong>Service Providers:</strong> We provide information to trusted partners who work on behalf of or with Q.S.T.A.R. Research under confidentiality agreements. These companies may use your personal information to help us communicate with you about offers from us and our marketing partners.</li>
        <li><strong>Legal Reasons:</strong> We will share personal information outside of Q.S.T.A.R. Research if we have a good-faith belief that access, use, preservation, or disclosure of the information is reasonably necessary to:
            <ul>
                <li>Meet any applicable law, regulation, legal process, or enforceable governmental request.</li>
                <li>Enforce applicable Terms of Service, including investigation of potential violations.</li>
                <li>Detect, prevent, or otherwise address fraud, security, or technical issues.</li>
                <li>Protect against harm to the rights, property, or safety of Q.S.T.A.R. Research, our users, or the public as required or permitted by law.</li>
            </ul>
        </li>
    </ul>

    <h3>4. Data Security</h3>
    <p>We strive to protect the confidentiality and security of the information it collects by employing appropriate technical and organizational measures designed to protect the information against unauthorized or unlawful processing and against accidental loss, destruction, or damage.</p>

    <h3>5. Your Rights</h3>
    <p>You have certain rights regarding the personal information we hold about you. These may include the right to access, correct, delete, or restrict the use of your personal information, as well as the right to withdraw consent to use your personal information. If you wish to exercise these rights, please contact us at the details provided below.</p>

    <h3>6. Changes to Our Privacy Policy</h3>
    <p>We may update this Privacy Statement from time to time. We will notify you of any changes by posting the new Privacy Statement on this page. We encourage you to review this Privacy Statement periodically for any changes.</p>

    <h3>Contact Us</h3>
    <p>If you have any questions about this Privacy Statement

  </div>

  <div data-hash="#/terms-of-use" class="terms-of-use">
    <h2>Terms of Use</h2>
    <p><strong>Effective Date:</strong> 2024-04-01</p>

    <p>Welcome to Q.S.T.A.R. Research. If you continue to browse and use this website, you are agreeing to comply with and be bound by the following terms and conditions of use, which together with our Privacy Policy govern Q.S.T.A.R. Research's relationship with you in relation to this website.</p>

    <h3>1. General Terms</h3>
    <p>By accessing this website, you are agreeing to be bound by these website Terms and Conditions of Use, all applicable laws and regulations, and agree that you are responsible for compliance with any applicable local laws. If you do not agree with any of these terms, you are prohibited from using or accessing this site.</p>

    <h3>2. Use License</h3>
    <p>
    Permission is granted to temporarily download one copy of the materials (information or software) on Q.S.T.A.R. Research's website for personal, non-commercial transitory viewing only. This is the grant of a license, not a transfer of title, and under this license you may not:
    <ul>
      <li>modify or copy the materials;</li>
      <li>use the materials for any commercial purpose, or for any public display (commercial or non-commercial);</li>
      <li>attempt to decompile or reverse engineer any software contained on Q.S.T.A.R. Research's website;</li>
      <li>remove any copyright or other proprietary notations from the materials; or</li>
      <li>transfer the materials to another person or "mirror" the materials on any other server.</li>
    </ul>
    This license shall automatically terminate if you violate any of these restrictions and may be terminated by Q.S.T.A.R. Research at any time. Upon terminating your viewing of these materials or upon the termination of this license, you must destroy any downloaded materials in your possession whether in electronic or printed format.
    </p>

    <h3>3. AGI/ASI Guardrails</h3>
    <p>As a condition of use, you are required to implement appropriate guardrails for Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI) to mitigate risks associated with these technologies. Q.S.T.A.R. Research is not liable for any damages that result from the failure to establish or maintain such guardrails, or from the risks inherent in the development, use, or deployment of AGI/ASI technologies.</p>

    <h3>4. Disclaimer</h3>
    <p>The materials on Q.S.T.A.R. Research's website are provided "as is". Q.S.T.A.R. Research makes no warranties, expressed or implied, and hereby disclaims and negates all other warranties, including without limitation, implied warranties or conditions of merchantability, fitness for a particular purpose, or non-infringement of intellectual property or other violation of rights. Further, Q.S.T.A.R. Research does not warrant or make any representations concerning the accuracy, likely results, or reliability of the use of the materials on its Internet site or otherwise relating to such materials or on any sites linked to this site.</p>

    <h3>5. Limitations</h3>
    <p>In no event shall Q.S.T.A.R. Research or its suppliers be liable for any damages (including, without limitation, damages for loss of data or profit, or due to business interruption,) arising out of the use or inability to use the materials on Q.S.T.A.R. Research's Internet site, even if Q.S.T.A.R. Research or a Q.S.T.A.R. Research authorized representative has been notified orally or in writing of the possibility of such damage.</p>

    <h3>6. Revisions and Errata</h3>
    <p>The materials appearing on Q.S.T.A.R. Research's website could include technical, typographical, or photographic errors. Q.S.T.A.R. Research does not warrant that any of the materials on its website are accurate, complete, or current. Q.S.T.A.R. Research may make changes to the materials contained on its website at any time without notice. Q.S.T.A.R. Research does not, however, make any commitment to update the materials.</p>
  </div>

  <div data-hash="#404" class="404">
      <h1>Page Not Found</h1>
    <p>
      This site is under heavy construction. Some pages are not fully designed yet. Please come back shortly to see if the content you're looking for has been added.
    </p>
  </div>

</div>

<footer>

  <div class="navs">
    <nav>
      <a href="https://twitter.com/gklain" target="_blank">Twitter</a>
      <a href="https://ch.linkedin.com/company/gianklain" target="_blank">LinkedIn</a>
      <a href="mailto:info@qstar-research.com" target="_blank">Email</a>
    </nav>
    <div class="spacer">&nbsp;</div>
    <nav>
      <a href="#/privacy">Privacy</a>
      <a href="#/terms-of-use">Terms of Use</a>
      <div>©QSTAR 2024</div>
    </nav>
  </div>

</footer>

<script type="text/javascript">

(async () => fetch('https://figur.li/l/?tag=loaded'))();

const toggle = document.querySelector('header .toggle');
const header = document.querySelector('header');
toggle.addEventListener('click', () => {
  header.classList.toggle('shown');
});

const els = [...document.querySelectorAll('*')].filter(e => e.dataset.hash);
const fouroufour = document.querySelector('[data-hash="#404"]');

function hashed() {
  let notfound = true;
  for (const el of els) {
    const found = el.dataset.hash === location.hash;
    el.style.display = found ? null : 'none';
    notfound &= !found;
  }
  fouroufour.style.display = notfound ? null : 'none';
  header.classList.remove('shown');
}

window.addEventListener('hashchange', hashed);

location.hash || (location.hash = '#/');
hashed();

</script>
